{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of CoFi for marine traffic modeling\n",
    "\n",
    "This notebook showcases how to use Consititional Filters (CoFi) in ProMis to improve modeling of ship movements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will auto-relead changed ProMis imports\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# We need some additional packages\n",
    "!pip install -q svgpath2mpl svgpathtools filterpy keplergl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import json\n",
    "from math import ceil, isfinite\n",
    "from pathlib import Path\n",
    "from time import monotonic\n",
    "from warnings import filterwarnings\n",
    "\n",
    "# Math\n",
    "import numpy as np\n",
    "from numpy import array, deg2rad, eye, mean, ndarray, pi, set_printoptions, stack\n",
    "from numpy.random import seed, standard_normal\n",
    "from scipy.stats import norm, uniform\n",
    "\n",
    "# Data Handling\n",
    "import pandas as pd\n",
    "from filterpy.monte_carlo import systematic_resample\n",
    "\n",
    "# Plotting\n",
    "import matplotlib\n",
    "from keplergl import KeplerGl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.transforms import Affine2D\n",
    "from seaborn import move_legend\n",
    "from svgpath2mpl import parse_path\n",
    "from svgpathtools import svg2paths\n",
    "import seaborn as sns\n",
    "\n",
    "# ProMis\n",
    "from promis import ProMis, StaRMap\n",
    "from promis.geo import (\n",
    "    CartesianCollection,\n",
    "    CartesianMap,\n",
    "    CartesianRasterBand,\n",
    "    PolarCollection,\n",
    "    PolarLocation,\n",
    ")\n",
    "from promis.loaders import NauticalChartLoader\n",
    "from promis.logic.spatial import Depth\n",
    "from promis.estimators.filters.particle import (\n",
    "    ParticleFilter,\n",
    "    gaussian_noise,\n",
    "    independent_sample,\n",
    "    squared_error,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use full width of juptyer notebook\n",
    "set_printoptions(linewidth=180, formatter={\"float_kind\": \"{:4.6f}\".format})\n",
    "matplotlib.rcParams[\"pdf.fonttype\"] = 42\n",
    "matplotlib.rcParams[\"ps.fonttype\"] = 42\n",
    "\n",
    "sns.set_theme(style=\"ticks\", rc={\"figure.figsize\": (8, 3), \"legend.title_fontsize\": 15})\n",
    "sns.set_style({\"font.family\": \"serif\", \"font.serif\": \"Times New Roman\"})\n",
    "\n",
    "filterwarnings(\"ignore\", module=\"tqdm.auto\", message=\"IProgress not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = \"New York Harbor\"\n",
    "bbox = (\n",
    "    # (lat, lon) lower left\n",
    "    (40.4, -74.1),\n",
    "    # (lat, lon) upper right\n",
    "    (40.7, -73.8),\n",
    ")\n",
    "\n",
    "# setting = \"Port of Charleston, South Carolina\"\n",
    "# bbox = (\n",
    "#     # (lat, lon) lower left\n",
    "#     (32.75, -79.97),\n",
    "#     # (lat, lon) upper right\n",
    "#     (32.875, -79.87),\n",
    "# )\n",
    "\n",
    "# setting = \"Baltimore\"\n",
    "# bbox = (\n",
    "#     # (lat, lon) lower left\n",
    "#     (39.174, -76.6304993),\n",
    "#     # (lat, lon) upper right\n",
    "#     (39.29, -76.4495423),\n",
    "# )\n",
    "\n",
    "# setting = \"Port of Virginia (Norfolk)\"\n",
    "# bbox = (\n",
    "#     # (lat, lon) lower left\n",
    "#     (36.7711453, -76.4994133),\n",
    "#     # (lat, lon) upper right\n",
    "#     (37.3020172, -75.8786833),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25468.03852707448, 33313.56617274144)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin = PolarLocation(\n",
    "    latitude=mean([bbox[0][0], bbox[1][0]]), longitude=mean([bbox[0][1], bbox[1][1]])\n",
    ")\n",
    "width = PolarLocation(latitude=bbox[0][0], longitude=bbox[0][1]).distance(\n",
    "    PolarLocation(latitude=bbox[0][0], longitude=bbox[1][1])\n",
    ")\n",
    "height = PolarLocation(latitude=bbox[0][0], longitude=bbox[0][1]).distance(\n",
    "    PolarLocation(latitude=bbox[1][0], longitude=bbox[0][1])\n",
    ")\n",
    "dimensions = width, height\n",
    "dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_resolution = (200, 200)\n",
    "support_resolution = (100, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load chart data\n",
    "\n",
    "This only needs to be done once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning 1: Illegal feature attribute id (ATTF:ATTL[0]) of 0\n",
      "on feature FIDN=295551429, FIDS=8527.\n",
      "Skipping attribute. No more warnings will be issued.\n",
      "Warning 1: Illegal feature attribute id (ATTF:ATTL[0]) of 0\n",
      "on feature FIDN=590831528, FIDS=8527.\n",
      "Skipping attribute. No more warnings will be issued.\n",
      "Warning 1: Illegal feature attribute id (ATTF:ATTL[0]) of 0\n",
      "on feature FIDN=834634418, FIDS=8527.\n",
      "Skipping attribute. No more warnings will be issued.\n"
     ]
    }
   ],
   "source": [
    "loader = NauticalChartLoader(\n",
    "    chart_root=Path(\".\").absolute().parent / \"data\" / \"us_charts-ny\",\n",
    "    origin=origin,\n",
    "    dimensions=dimensions,\n",
    ")\n",
    "loader.load(n_jobs=16)\n",
    "\n",
    "uam = loader.to_cartesian_map()\n",
    "uam.apply_covariance(20.0 * eye(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded {len(uam.features)} features\")\n",
    "print()\n",
    "\n",
    "# for feature in uam.features:\n",
    "#     # Water is visualized below anyways\n",
    "#     if feature.location_type != \"water\":\n",
    "#         print(f\"{type(feature).__name__} - {feature.location_type}: \\t{feature.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uam.save(\"nautical_chart_uam.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the mission landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic = r\"\"\"\n",
    "% Background knowledge\n",
    "is_deep(X) :- A is depth(X, water), B is current_draft , A + B < -1, \\+ over(X, land).\n",
    "\n",
    "0.95::is_safe(X) :- is_deep(X), distance(X, land) > 50. %, \\+ over(X, anchorage).\n",
    "\n",
    "bound_to_main_routes :- is_cargo ; is_cargo_hazardous ;\n",
    "    is_tanker ; is_tanker_hazardous ; length > 100.\n",
    "follows_main_routes(X) :- \\+  bound_to_main_routes ; distance(X, water_route) < 400 ;\n",
    "    distance(X, water_route) > 2000.\n",
    "%is_sane_speed(X) :- \\+ over(X, anchorage) ; speed_over_ground < 10.\n",
    "is_legal(X) :- follows_main_routes(X). %, is_sane_speed(X).\n",
    "\n",
    "is_useful(X) :- \\+ is_search_and_rescue_vessels.\n",
    "0.2::is_useful(X) :- is_search_and_rescue_vessels.\n",
    "0.8::is_useful(X) :- is_search_and_rescue_vessels, distance(X, land) < 500.\n",
    "\n",
    "% This rule set does not model all possible bahavioral patterns of ships\n",
    "%0.1::is_ignorant.\n",
    "landscape(X) :- is_safe(X), is_legal(X), is_useful(X). % ; is_ignorant.\n",
    "% landscape(X) :- is_deep(X).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a UAM\n",
    "uam = CartesianMap.load(\"nautical_chart_uam.pkl\")\n",
    "\n",
    "# We create a statistical relational map (StaR Map) to represent the\n",
    "# stochastic relationships in the environment, computing a raster of 100 x 100 points\n",
    "# using linear interpolation of a sample set\n",
    "before = monotonic()\n",
    "target = CartesianRasterBand(origin, target_resolution, width, height)\n",
    "star_map = StaRMap(target, uam, method=\"nearest\")\n",
    "\n",
    "star_map.initialize(\n",
    "    # The sample points for which the relations will be computed directly\n",
    "    support=CartesianRasterBand(origin, support_resolution, width, height),\n",
    "    # We now compute the Distance and Over relationships for the selected points\n",
    "    # For this, we take many random samples from generated/possible map variations\n",
    "    number_of_random_maps=5,  # TODO 25\n",
    "    # The logic used later. This is required to determine which relationships to prepare\n",
    "    logic=logic,\n",
    ")\n",
    "after = monotonic()\n",
    "\n",
    "star_map.save(\"nautical_chart_star_map.pkl\")\n",
    "\n",
    "runtime = after - before\n",
    "f\"Computed StaR Map in {runtime} seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_map = StaRMap.load(\"nautical_chart_star_map.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first plot the depth since we have some special handling for it\n",
    "depth: Depth = star_map.get(\"depth\", \"water\")\n",
    "plt.title(\"Depth (mean)\")\n",
    "depth.plot(value_index=0)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Depth (variance)\")\n",
    "depth.plot(value_index=1)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_relations = star_map.relation_and_location_types\n",
    "all_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one row per relation type and one column per location type\n",
    "fig, axes = plt.subplots(\n",
    "    len(all_relations), len(star_map.location_types), figsize=(15, 5), sharex=True, sharey=True\n",
    ")\n",
    "\n",
    "for i, (relation_type, location_types) in enumerate(all_relations.items()):\n",
    "    for j, location_type in enumerate(star_map.location_types):\n",
    "        # Get the axis for the current row and column\n",
    "        ax = axes[i, j]\n",
    "\n",
    "        # Label the rows and columns\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(relation_type)\n",
    "        if i == 0:\n",
    "            ax.set_title(location_type)\n",
    "\n",
    "        # Get the relation and visualize it\n",
    "        if location_type in location_types:\n",
    "            relation = star_map.get(relation_type, location_type)\n",
    "            relation.parameters.scatter(ax=ax, cmap=\"coolwarm\", s=0.1)\n",
    "\n",
    "        # if j == len(all_location_types) - 1:\n",
    "        #     plt.colorbar()\n",
    "\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation = star_map.get(\"over\", \"anchorage\")\n",
    "relation.parameters.scatter(vmin=0, s=0.1)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = Path(f\"export-relations-{setting}\")\n",
    "folder.mkdir(exist_ok=True)\n",
    "\n",
    "for relation_type, location_types in star_map.relation_and_location_types.items():\n",
    "    for location_type in location_types:\n",
    "        relation = star_map.get(relation_type, location_type)\n",
    "        path = folder / f\"{relation_type}_{location_type}.csv\"\n",
    "        relation.parameters.to_polar().data.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run some inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In ProMis, we define the constraints of the mission\n",
    "# as hybrid probabilistic first-order logic programs\n",
    "\n",
    "logic = \"\"\"\n",
    "landscape(X) :- over(X, water_route), depth(X, water) < -5.\n",
    "\"\"\"\n",
    "\n",
    "# Solve mission constraints using StaRMap parameters and multiprocessing\n",
    "promis = ProMis(star_map, logic)\n",
    "landscape = promis.solve(n_jobs=12, batch_size=10, show_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the resulting landscape\n",
    "landscape.scatter()\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promis.save(\"nautical_chart_promis.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the landscape for specific locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promis = ProMis.load(\"nautical_chart_promis.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_at(promis: ProMis, query_locations: ndarray) -> CartesianCollection:\n",
    "    \"\"\"Solve the landscape at specific query locations.\n",
    "\n",
    "    Note:\n",
    "        Modifies the target of the ProMis object to the query locations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate the query location shape\n",
    "    assert len(query_locations.shape) == 2, (\n",
    "        f\"Query locations must be 2D, got {query_locations.shape}\"\n",
    "    )\n",
    "    assert query_locations.shape[1] == 2, \"Query locations must be 2D\"\n",
    "\n",
    "    # Update the target of the ProMis object to the query locations\n",
    "    new_target = CartesianCollection(\n",
    "        origin=star_map.uam.origin, number_of_values=star_map.target.dimensions\n",
    "    )\n",
    "    new_target.append_with_default(query_locations, value=0.0)\n",
    "    promis.star_map.target = new_target\n",
    "\n",
    "    # Solve the ProMis inference problem at the target locations\n",
    "    return promis.solve(n_jobs=12, batch_size=10, show_progress=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showcase the solution at specific locations\n",
    "result = solve_at(promis, query_locations=array([[-1300, 200], [0, 0], [2000, 2000]]))\n",
    "\n",
    "# Show the resulting landscape\n",
    "result.scatter(cmap=\"flare\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "# The resulting datafram retains the order of the query locations\n",
    "# The relevant inference solution could now be extracted using the `v0` key\n",
    "result.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load some AIS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_types = json.loads((Path(\"..\") / \"data\" / \"vessel_types_simplified.json\").read_text())\n",
    "# vessel_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ais(\n",
    "    path: str | Path, bbox: None | tuple[tuple[float, float], tuple[float, float]]\n",
    ") -> pd.DataFrame:\n",
    "    # Read the csv into pandas\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Filter by bounding box\n",
    "    if bbox is not None:\n",
    "        # This is a overly simplistic bounding box filter that only works on moderate latitudes\n",
    "        # and far from the dateline\n",
    "        df = df[\n",
    "            (df[\"LAT\"] > bbox[0][0])\n",
    "            & (df[\"LAT\"] < bbox[1][0])\n",
    "            & (df[\"LON\"] > bbox[0][1])\n",
    "            & (df[\"LON\"] < bbox[1][1])\n",
    "        ]\n",
    "\n",
    "    # Sort by time per ship\n",
    "    df[\"BaseDateTime\"] = pd.to_datetime(df[\"BaseDateTime\"])\n",
    "    df.sort_values([\"MMSI\", \"BaseDateTime\"], inplace=True)\n",
    "\n",
    "    df.loc[df[\"VesselType\"].isna(), \"VesselType\"] = 0  # Zero means unknown\n",
    "\n",
    "    df[\"VesselType\"] = df[\"VesselType\"].astype(int)\n",
    "    df[\"VesselTypeName\"] = df[\"VesselType\"].astype(str).map(vessel_types)\n",
    "\n",
    "    # Filter out all vessels that have an average SOG of less than 1 knot\n",
    "    df = df.groupby(\"MMSI\").filter(lambda x: x[\"SOG\"].mean() > 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_ais(Path(\"..\") / \"data\" / \"ais\" / \"AIS_2023_08_01.csv\", bbox=bbox)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = PolarCollection(origin=origin, dimensions=0)\n",
    "collection.append_with_default(df[[\"LON\", \"LAT\"]].values, value=())\n",
    "cartesian = collection.to_cartesian()\n",
    "\n",
    "df.loc[:, \"East\"] = cartesian.data[\"east\"].values\n",
    "df.loc[:, \"North\"] = cartesian.data[\"north\"].values\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"MMSI\"].value_counts().hist(bins=100)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MMSIs = df[\"MMSI\"].value_counts()\n",
    "min_observations = 10 + 20 + 15  # warmup + some usage in between + dead_reckoning\n",
    "relevant_MMSIs = MMSIs[MMSIs > min_observations].index\n",
    "df = df[df[\"MMSI\"].isin(relevant_MMSIs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_type_to_index = {\n",
    "    vessel_type: i for i, vessel_type in enumerate(sorted(df[\"VesselTypeName\"].unique()))\n",
    "}\n",
    "palette = sns.color_palette(\"tab20\", len(vessel_type_to_index))\n",
    "\n",
    "relevant_vessel_types = df[\"VesselTypeName\"].unique()\n",
    "\n",
    "cols = 5\n",
    "all_rows = ceil(len(relevant_vessel_types) / cols)\n",
    "fig, ax = plt.subplots(ncols=cols, nrows=all_rows, figsize=(13, 8), sharex=True, sharey=True)\n",
    "\n",
    "for (vessel_type, i), ax in zip(vessel_type_to_index.items(), ax.flatten()):\n",
    "    for mmsi in relevant_MMSIs:\n",
    "        df_ = df[df[\"MMSI\"] == mmsi]\n",
    "        if relevant_vessel_types is not None and df_[\"VesselTypeName\"].iloc[0] != vessel_type:\n",
    "            continue\n",
    "        vessel_type = df_[\"VesselTypeName\"].iloc[0]\n",
    "        color = palette[vessel_type_to_index[vessel_type]]\n",
    "        ax.plot(df_[\"LON\"], df_[\"LAT\"], c=color)\n",
    "        # plt.scatter(df_[\"LON\"], df_[\"LAT\"], c=df_[\"SOG\"], cmap=\"viridis\", s=8)\n",
    "\n",
    "    # plt.colorbar()\n",
    "\n",
    "    ax.set_xlim(bbox[0][1], bbox[1][1])\n",
    "    if i // cols == all_rows - 1:\n",
    "        ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylim(bbox[0][0], bbox[1][0])\n",
    "    if i % cols == 0:\n",
    "        ax.set_ylabel(\"Latitude\")\n",
    "    # ax.set_aspect(\"equal\")\n",
    "\n",
    "    # if the center right ahdn side\n",
    "    col = i % cols\n",
    "    row = i // cols\n",
    "    if col == cols - 1 and row == len(relevant_vessel_types) // cols - 1:\n",
    "        ax.legend(\n",
    "            [\n",
    "                plt.Line2D([0], [0], color=palette[i], lw=2, label=vessel_type)\n",
    "                for i, vessel_type in enumerate(vessel_type_to_index)\n",
    "            ],\n",
    "            vessel_type_to_index.keys(),\n",
    "            title=\"Vessel Type\",\n",
    "            loc=\"center left\",\n",
    "            bbox_to_anchor=(1.05, 0.5),\n",
    "        )\n",
    "\n",
    "plt.suptitle(setting)\n",
    "\n",
    "(Path(\".\") / \"plots\").mkdir(exist_ok=True)\n",
    "plt.savefig(Path(\".\") / \"plots\" / f\"ais-{setting}-some.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_type = \"Search and rescue vessels\"\n",
    "df_ = df[df[\"VesselTypeName\"] == vessel_type]\n",
    "path = Path(\".\") / \"plots\" / f\"ais-{setting}-trace\"\n",
    "path.mkdir(exist_ok=True)\n",
    "df_[[\"LON\", \"LAT\", \"MMSI\"]].to_csv(path / f\"{vessel_type}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_vessel_types = [\n",
    "    \"Cargo\",\n",
    "    \"Cargo hazardous\",\n",
    "    \"Search and rescue vessels\",\n",
    "    \"Towing\",\n",
    "    \"Other\",\n",
    "    \"Tanker\",\n",
    "    \"Tanker hazardous\",\n",
    "    # \"Passenger\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mmsi in relevant_MMSIs:\n",
    "#     df_ = df[df[\"MMSI\"] == mmsi]\n",
    "#     if df_[\"VesselTypeName\"].iloc[0] not in relevant_vessel_types:\n",
    "#         continue\n",
    "\n",
    "#     plt.plot(df_[\"LON\"], df_[\"LAT\"], c=color)\n",
    "#     plt.xlim(bbox[0][1], bbox[1][1])\n",
    "#     plt.ylim(bbox[0][0], bbox[1][0])\n",
    "#     plt.gca().set_aspect(\"equal\")\n",
    "\n",
    "#     path = Path(\".\") / \"plots\" / f\"ais-{setting}\" / df_[\"VesselTypeName\"].iloc[0]\n",
    "#     path.mkdir(exist_ok=True, parents=True)\n",
    "#     plt.savefig(path / f\"{mmsi}.pdf\", bbox_inches=\"tight\")\n",
    "#     plt.clf()\n",
    "\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_mmsi = 636017103\n",
    "track_mmsi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the landscape to filter with agent constitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_marker = parse_path(svg2paths(\"ship-icon.svg\")[1][0][\"d\"])\n",
    "ship_marker.vertices -= ship_marker.vertices.mean(axis=0)\n",
    "ship_marker = ship_marker.transformed(Affine2D().rotate_deg(180)).transformed(\n",
    "    Affine2D().scale(-1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df[df[\"MMSI\"] == track_mmsi]\n",
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SOG\"].hist(bins=100)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.arange(len(df_[\"RelativeTime\"])), df_[\"RelativeTime\"].diff())\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load it again since we modified it\n",
    "promis.star_map = StaRMap.load(\"nautical_chart_star_map.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def landscape_for_mmsi(mmsi: int) -> CartesianCollection:\n",
    "    metadata = df[df[\"MMSI\"] == mmsi].iloc[0]\n",
    "\n",
    "    vessel_type = metadata[\"VesselTypeName\"].replace(\" \", \"_\").lower()\n",
    "    logic += f\"\\nis_{vessel_type}.\"\n",
    "    for vessel_type in relevant_vessel_types:\n",
    "        if vessel_type != metadata[\"VesselTypeName\"]:\n",
    "            logic += f\"\\n0::is_{vessel_type.replace(' ', '_').lower()}.\"\n",
    "\n",
    "    length = metadata[\"Length\"]\n",
    "    if isfinite(length) and length > 0:\n",
    "        logic += f\"\\nlength ~ normal({length}, {length * 0.05}).\"\n",
    "    else:\n",
    "        # Some prior knowledge\n",
    "        logic += \"\\nlength ~ normal(30, 10).\"\n",
    "\n",
    "    draft = metadata[\"Draft\"]\n",
    "    if isfinite(length) and draft > 0:\n",
    "        logic += f\"\\ncurrent_draft ~ normal({draft}, {draft * 0.1}).\"\n",
    "    else:\n",
    "        # Some prior knowledge\n",
    "        logic += \"\\ncurrent_draft ~ normal(2, 1).\"\n",
    "\n",
    "    # TODO: Time varying!\n",
    "    # speed_over_ground = metadata[\"SOG\"]\n",
    "    # if isfinite(speed_over_ground):\n",
    "    #     logic += f\"\\nspeed_over_ground ~ normal({speed_over_ground}, {speed_over_ground*0.1}).\"\n",
    "    # else:\n",
    "    #     # Some prior knowledge\n",
    "    #     logic += \"\\nspeed_over_ground ~ normal(5, 5).\"\n",
    "\n",
    "    logic += \"\\n\\n\"  # Add newlines for better readability\n",
    "\n",
    "    promis = ProMis(star_map, logic)\n",
    "    landscape = promis.solve(n_jobs=12, batch_size=5, show_progress=False)\n",
    "\n",
    "    return landscape\n",
    "\n",
    "\n",
    "landscapes = {\n",
    "    \"Cargo\": landscape_for_mmsi(636017103),\n",
    "    # \"Search and rescue vessels\": landscape_for_mmsi(367531710),\n",
    "    # \"Towing\": landscape_for_mmsi(303461000),\n",
    "}\n",
    "landscape = landscapes[\"Cargo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landscape.scatter()\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\".\") / \"plots\" / f\"ais-{setting}-trace\"\n",
    "\n",
    "for vessel_type, landscape in landscapes.items():\n",
    "    polar_collection = landscape.to_polar()\n",
    "    polar_collection.data.to_csv(path / f\"{vessel_type}-landscape.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vessel_type, landscape in landscapes.items():\n",
    "    landscape.scatter()\n",
    "    plt.colorbar()\n",
    "    plt.title(vessel_type)\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolators = {\n",
    "    vessel_type: landscape.get_interpolator(method=\"nearest\")\n",
    "    for vessel_type, landscape in landscapes.items()\n",
    "}\n",
    "interpolator = interpolators[\"Cargo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typical_ship_speed = 5  # meters per second\n",
    "typical_sample_time = 60  # seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation_duration = df_[\"RelativeTime\"].iloc[-1].item()\n",
    "# num_simulation_steps = len(df_[\"RelativeTime\"])\n",
    "# num_simulation_steps, simulation_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectories(all_positions: ndarray, label: str | None = \"Trajectory {}\", **kwargs):\n",
    "    if len(all_positions.shape) == 2:\n",
    "        all_positions = all_positions[None, ...]\n",
    "    for i, positions in enumerate(all_positions):\n",
    "        plt.plot(*zip(*positions), label=label.format(i) if label else None, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "landscape.scatter(s=0.2, alpha=0.5)\n",
    "# plot_trajectories(stack((df_[\"East\"], df_[\"North\"])), label=\"True trajectory\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectories_animated(positions, filename: str = \"trajectory.gif\") -> None:\n",
    "    fig, _ = plt.subplots()\n",
    "    landscape.scatter()\n",
    "    (line,) = plt.plot(*zip(*positions), label=\"Ship Position\")\n",
    "    plt.colorbar()\n",
    "\n",
    "    def update(num, x, y, line):\n",
    "        line.set_data(x[:num], y[:num])\n",
    "        line.axes.axis([-width / 2, width / 2, -height / 2, height / 2])\n",
    "        return (line,)\n",
    "\n",
    "    ani = FuncAnimation(\n",
    "        fig,\n",
    "        update,\n",
    "        len(positions),\n",
    "        fargs=[positions[:, 0], positions[:, 1], line],\n",
    "        interval=1000 / 60,\n",
    "        blit=True,\n",
    "    )\n",
    "    ani.save(filename)\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "# plot_trajectories_animated(ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old particle filter code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    data: pd.DataFrame,\n",
    "    dead_reckoning_steps: int = 15,\n",
    "    verbose: bool = False,\n",
    "    seed_value: int = 2024,\n",
    "    eval_skip_warmup: int = 10,\n",
    "    with_constitution: bool = True,\n",
    "    constitutional_trust: float = 0.8,\n",
    ") -> dict:\n",
    "    seed(seed_value)\n",
    "\n",
    "    # We define the first and second derivatives in polar coordinates\n",
    "    # since that is how the AIS data is given (for boat speed and direction)\n",
    "    # and is is quite natural for the problem at hand (ships change direction\n",
    "    # more often than speed)\n",
    "    # column_names = [\"x\", \"y\", \"dphi\", \"dr\"]  # , \"ddphi\", \"ddr\"]\n",
    "    column_names = [\"x\", \"y\", \"dx\", \"dy\"]  # , \"ddphi\", \"ddr\"]\n",
    "\n",
    "    # prior sampling function for each variable\n",
    "    prior_fn = independent_sample(\n",
    "        [\n",
    "            uniform(loc=-width / 2, scale=width).rvs,\n",
    "            uniform(loc=-height / 2, scale=height).rvs,\n",
    "            # uniform(loc=0, scale=2 * pi).rvs,\n",
    "            # norm(loc=typical_ship_speed, scale=typical_ship_speed).rvs,\n",
    "            norm(loc=0, scale=typical_ship_speed).rvs,\n",
    "            norm(loc=0, scale=typical_ship_speed).rvs,\n",
    "            # norm(loc=0, scale=0.01 * 2 * pi).rvs,\n",
    "            # norm(loc=0, scale=0.01 * typical_ship_speed).rvs,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def polar_to_cartesian(phi: ndarray, r: ndarray) -> ndarray:\n",
    "        \"\"\"Convert polar coordinates to cartesian.\"\"\"\n",
    "        return stack((r * np.sin(phi), r * np.cos(phi)), axis=1)\n",
    "\n",
    "    dt = None\n",
    "\n",
    "    def state_forward_model(x: ndarray) -> ndarray:\n",
    "        \"\"\"Very simple linear dynamics: pos += dpos and dpos += ddpos.\"\"\"\n",
    "        nonlocal dt\n",
    "\n",
    "        xp = np.array(x)\n",
    "\n",
    "        # However, we need to convert the polar coordinates to cartesian\n",
    "        # dxy = polar_to_cartesian(xp[:, 2], xp[:, 3])\n",
    "        # xp[:, 0] += dxy[:, 0] * dt\n",
    "        # xp[:, 1] += dxy[:, 1] * dt\n",
    "\n",
    "        xp[:, [0, 1]] += xp[:, [2, 3]] * dt\n",
    "\n",
    "        # xp[:, 2] %= 2 * pi\n",
    "        # xp[:, 3] = clip(xp[:, 3], 0, None)\n",
    "\n",
    "        # First and second derivatives are both in polar coordinates\n",
    "        # xp[:, [2, 3]] += xp[:, [4, 5]] * dt\n",
    "\n",
    "        return xp\n",
    "\n",
    "    trace_weight_e1 = []\n",
    "    trace_weight_e2 = []\n",
    "\n",
    "    def weight_fn(hyp, obs) -> ndarray:\n",
    "        nonlocal trace_weight_e1, trace_weight_e2, with_constitution, constitutional_trust\n",
    "\n",
    "        # Make sure everything is in cartesian coordinates for proper distance computation\n",
    "        # Also make sure that the rough scales are the same\n",
    "        # hyp = hyp.copy()\n",
    "        # hyp[:, [2, 3]] = polar_to_cartesian(hyp[:, 2], hyp[:, 3])\n",
    "        # hyp[:, 2] /= typical_ship_speed\n",
    "        # hyp[:, 3] /= typical_ship_speed\n",
    "        # obs = obs.copy()\n",
    "        # obs[:, [2, 3]] = polar_to_cartesian(obs[:, 2], obs[:, 3])\n",
    "        # obs[:, 2] /= typical_ship_speed\n",
    "        # obs[:, 3] /= typical_ship_speed\n",
    "\n",
    "        # Compute by distance to observations\n",
    "        # speed = obs[0, 3]\n",
    "        # from numpy import clip\n",
    "        # speed = clip(speed, 0.5, 10)\n",
    "        # e1 = squared_error(hyp[:, :2], obs[:, :2], sigma=typical_sample_time * speed * 0.5)\n",
    "        # e2 = squared_error(hyp[:, 2:], obs[:, 2:], sigma=speed * 0.2)\n",
    "        # weight = e1 * e2\n",
    "\n",
    "        weight = squared_error(hyp[:, :2], obs[:, :2], sigma=5)\n",
    "\n",
    "        trace_weight_e1.append(-1)\n",
    "        trace_weight_e2.append(-1)\n",
    "\n",
    "        if with_constitution:\n",
    "            # constitution = solve_at(promis, hyp[:, :2]).data[\"v0\"].to_numpy()\n",
    "            constitution = interpolator(hyp[:, :2])[:, 0]\n",
    "\n",
    "            # plt.hist(from_landscape)\n",
    "            # plt.xlabel(\"Landscape Value\")\n",
    "            # plt.ylabel(\"Frequency\")\n",
    "            # plt.show()\n",
    "            # plt.clf()\n",
    "\n",
    "            weight *= constitutional_trust * constitution + (1 - constitutional_trust)\n",
    "        return weight\n",
    "\n",
    "    # create the filter\n",
    "    pf = ParticleFilter(\n",
    "        prior_fn=prior_fn,\n",
    "        observe_fn=lambda x: x[:, :4],\n",
    "        n_particles=2000,\n",
    "        dynamics_fn=state_forward_model,\n",
    "        noise_fn=lambda x: gaussian_noise(\n",
    "            x,\n",
    "            sigmas=[\n",
    "                5,\n",
    "                5,\n",
    "                0.2,\n",
    "                0.2,\n",
    "                # typical_ship_speed * typical_sample_time / 0.5,\n",
    "                # typical_ship_speed * typical_sample_time / 0.5,\n",
    "                # 0.03 * 2 * pi,\n",
    "                # typical_ship_speed * 0.05,\n",
    "                # typical_ship_speed * 0.5,\n",
    "                # typical_ship_speed * 0.5,\n",
    "                # 0.02**2 * 2 * pi,\n",
    "                # typical_ship_speed * 0.05**2,\n",
    "            ],\n",
    "        ),\n",
    "        weight_fn=weight_fn,\n",
    "        resample_proportion=0.01,\n",
    "        n_eff_threshold=0.5,\n",
    "        column_names=column_names,\n",
    "    )\n",
    "\n",
    "    trace_time = []\n",
    "    trace_all_particles = [pf.particles.copy()]  # only here we already have the initial particles\n",
    "    trace_state = []\n",
    "    trace_map_state = []\n",
    "    trace_hypotheses = []\n",
    "    trace_map_hypotheses = []\n",
    "    trace_weights = []\n",
    "    trace_weights_unnormalized = []\n",
    "\n",
    "    data.loc[:, \"RelativeTime\"] = (\n",
    "        data[\"BaseDateTime\"] - data[\"BaseDateTime\"].iloc[0]\n",
    "    ).dt.total_seconds()\n",
    "    assert data[\"RelativeTime\"].is_monotonic_increasing\n",
    "\n",
    "    ground_truth = stack((data[\"East\"], data[\"North\"], deg2rad(data[\"COG\"]), data[\"SOG\"])).T\n",
    "    ground_truth[:, [2, 3]] = polar_to_cartesian(ground_truth[:, 2], ground_truth[:, 3])\n",
    "    deviate_from = len(ground_truth) - dead_reckoning_steps\n",
    "\n",
    "    for i, obs in enumerate(ground_truth):\n",
    "        if i == 0:\n",
    "            dt = 0\n",
    "        else:\n",
    "            dt = data[\"RelativeTime\"].iloc[i] - data[\"RelativeTime\"].iloc[i - 1]\n",
    "        trace_time.append(data[\"RelativeTime\"].iloc[i])\n",
    "\n",
    "        if i < deviate_from:\n",
    "            pf.update(obs)\n",
    "        else:\n",
    "            pf.update(None)\n",
    "\n",
    "        if verbose and i % 10 == 0:\n",
    "            print(f\"Step {i + 1}/{len(ground_truth)}\")\n",
    "\n",
    "        trace_all_particles.append(pf.particles.copy())\n",
    "        trace_state.append(pf.mean_state)\n",
    "        trace_map_state.append(pf.map_state)\n",
    "        trace_hypotheses.append(pf.mean_hypothesis)\n",
    "        trace_map_hypotheses.append(pf.map_hypothesis)\n",
    "        trace_weights.append(pf.weights)\n",
    "        trace_weights_unnormalized.append(pf.weights_unnormalized)\n",
    "\n",
    "    trace_all_particles = array(trace_all_particles)\n",
    "    trace_state = array(trace_state)\n",
    "    trace_map_state = array(trace_map_state)\n",
    "    trace_hypotheses = array(trace_hypotheses)\n",
    "    trace_map_hypotheses = array(trace_map_hypotheses)\n",
    "    trace_weights = array(trace_weights)\n",
    "    trace_weights_unnormalized = array(trace_weights_unnormalized)\n",
    "    trace_weight_e1 = array(trace_weight_e1)\n",
    "    trace_weight_e2 = array(trace_weight_e2)\n",
    "\n",
    "    # Evaluate the filter\n",
    "    position_accuracy_mean = np.linalg.norm(trace_state[:, :2] - ground_truth[:, :2], axis=1)[\n",
    "        eval_skip_warmup:\n",
    "    ]\n",
    "    position_accuracy_map = np.linalg.norm(trace_map_state[:, :2] - ground_truth[:, :2], axis=1)[\n",
    "        eval_skip_warmup:\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"trace_time\": trace_time,\n",
    "        \"deviate_from\": deviate_from,\n",
    "        \"trace_all_particles\": trace_all_particles,\n",
    "        \"trace_state\": trace_state,\n",
    "        \"trace_map_state\": trace_map_state,\n",
    "        \"trace_hypotheses\": trace_hypotheses,\n",
    "        \"trace_map_hypotheses\": trace_map_hypotheses,\n",
    "        \"trace_weights\": trace_weights,\n",
    "        \"trace_weights_unnormalized\": trace_weights_unnormalized,\n",
    "        \"trace_weight_e1\": trace_weight_e1,\n",
    "        \"trace_weight_e2\": trace_weight_e2,\n",
    "        \"particle_filter\": pf,\n",
    "        \"position_accuracy_mean\": position_accuracy_mean,\n",
    "        \"position_accuracy_map\": position_accuracy_map,\n",
    "    }\n",
    "\n",
    "\n",
    "debug_df = df[df[\"MMSI\"] == relevant_MMSIs[6]].copy()\n",
    "# result = run_experiment(debug_df.iloc[:200], constitutional_trust=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i >= result[\"trace_all_particles\"].shape[2]:\n",
    "        break\n",
    "    ax.plot(result[\"trace_all_particles\"][:, :, i], alpha=0.2)\n",
    "    ax.plot(result[\"trace_state\"][:, i], c=\"black\", lw=2, label=\"Mean\")\n",
    "    ax.plot(result[\"trace_map_state\"][:, i], c=\"red\", lw=2, label=\"MAP\")\n",
    "    if i < result[\"ground_truth\"].shape[1]:\n",
    "        ax.plot(result[\"ground_truth\"][:, i], c=\"green\", lw=2, label=\"Ground Truth\")\n",
    "    ax.set_title(result[\"particle_filter\"].column_names[i])\n",
    "\n",
    "    # ax.axvline(result[\"deviate_from\"], c=\"black\", ls=\"--\", lw=1)\n",
    "\n",
    "    if i == 0:\n",
    "        ax.set_ylim(-width, width)\n",
    "    if i == 1:\n",
    "        ax.set_ylim(-height, height)\n",
    "\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "\n",
    "axes[1, 0].axhline(2 * pi, c=\"black\", ls=\"--\", lw=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle filtering on our own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_pf1(\n",
    "    data: ndarray,\n",
    "    seed_value: int = 2024,\n",
    "    constitutional_trust: float = 0.8,\n",
    "    interpolator=None,\n",
    "    full_inference: bool = False,\n",
    "    use_constitution: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    def create_initial_particles(N: int) -> ndarray:\n",
    "        return np.stack(\n",
    "            [\n",
    "                uniform(loc=-width / 2, scale=width).rvs(N),\n",
    "                uniform(loc=-height / 2, scale=height).rvs(N),\n",
    "                uniform(loc=0, scale=2 * pi).rvs(N),\n",
    "                norm(loc=5, scale=3).rvs(N),\n",
    "                # norm(loc=0, scale=typical_ship_speed).rvs(N),\n",
    "                # norm(loc=0, scale=typical_ship_speed).rvs(N),\n",
    "            ]\n",
    "        ).T\n",
    "\n",
    "    def predict(particles, dt=1.0):\n",
    "        \"\"\"move according to control input u (heading change, velocity)\n",
    "        with noise Q (std heading change, std velocity)`\"\"\"\n",
    "\n",
    "        particles = particles.copy()\n",
    "\n",
    "        N = len(particles)\n",
    "\n",
    "        # noise = standard_normal(N) * std[1]\n",
    "        # particles[:, [0, 1]] += particles[:, [2, 3]] * dt + noise\n",
    "\n",
    "        particles[:, 2] += standard_normal(N) * 0.05\n",
    "        particles[:, 2] %= 2 * pi\n",
    "\n",
    "        particles[:, 3] += standard_normal(N) * 0.3\n",
    "\n",
    "        dist = particles[:, 3] * dt  # + (standard_normal(N) * std)\n",
    "\n",
    "        # TODO: possibly also add noise to the heading\n",
    "        particles[:, 0] += np.cos(particles[:, 2]) * dist\n",
    "        particles[:, 1] += np.sin(particles[:, 2]) * dist\n",
    "\n",
    "        return particles\n",
    "\n",
    "    def update(\n",
    "        particles,\n",
    "        weights,\n",
    "        z,\n",
    "        R,\n",
    "        use_constitution: bool = use_constitution,\n",
    "        constitutional_trust: float = constitutional_trust,\n",
    "    ):\n",
    "        weights = weights.copy()\n",
    "\n",
    "        positions = particles[:, :2]\n",
    "        distance = np.linalg.norm(positions - z, axis=1)\n",
    "\n",
    "        # Evaluate and RBF kernel\n",
    "        weights *= np.exp(-(distance**2) / (2 * R**2))\n",
    "\n",
    "        if use_constitution:\n",
    "            if full_inference:\n",
    "                # TODO this will likely use the wrong interpolator\n",
    "                constitution = solve_at(promis, positions).data[\"v0\"].to_numpy()\n",
    "            else:\n",
    "                constitution = interpolator(positions)[:, 0]\n",
    "            weights *= constitutional_trust * constitution + (1 - constitutional_trust)\n",
    "\n",
    "        weights += 1.0e-300  # avoid round-off to zero\n",
    "        weights /= sum(weights)  # normalize\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def estimate(particles, weights):\n",
    "        \"\"\"returns mean and variance of the weighted particles\"\"\"\n",
    "\n",
    "        mean = np.average(particles, weights=weights, axis=0)\n",
    "        var = np.average((particles - mean) ** 2, weights=weights, axis=0)\n",
    "        return mean, var\n",
    "\n",
    "    def neff(weights):\n",
    "        return 1.0 / np.sum(np.square(weights))\n",
    "\n",
    "    def resample_from_index(particles, weights, indexes):\n",
    "        particles[:] = particles[indexes]\n",
    "        weights.resize(len(particles))\n",
    "        weights.fill(1.0 / len(weights))\n",
    "\n",
    "    def run_pf1(\n",
    "        N,\n",
    "        data,\n",
    "        sensor_std_err=5000,\n",
    "        do_plot=False,\n",
    "        plot_particles=False,\n",
    "    ):\n",
    "        if do_plot:\n",
    "            plt.figure()\n",
    "\n",
    "        # create particles\n",
    "        particles = create_initial_particles(N)\n",
    "        particles[:, 0] = norm(loc=data[0, 0], scale=1000).rvs(N)\n",
    "        particles[:, 1] = norm(loc=data[0, 1], scale=1000).rvs(N)\n",
    "\n",
    "        # create weights\n",
    "        weights = np.ones(N) / N\n",
    "\n",
    "        if do_plot and plot_particles:\n",
    "            alpha = 0.20\n",
    "            if N > 5000:\n",
    "                alpha *= np.sqrt(5000) / np.sqrt(N)\n",
    "            plt.scatter(particles[:, 0], particles[:, 1], alpha=alpha, color=\"g\")\n",
    "\n",
    "        weights_trace = []\n",
    "        particle_trace = []\n",
    "        xs = []\n",
    "        for i, obs in enumerate(data):\n",
    "            # move\n",
    "            particles = predict(particles, dt=obs[4])\n",
    "\n",
    "            # incorporate measurements\n",
    "            weights = update(particles, weights, z=obs[:2], R=sensor_std_err)\n",
    "\n",
    "            # resample if too few effective particles\n",
    "            if neff(weights) < N / 2:\n",
    "                indexes = systematic_resample(weights)\n",
    "                resample_from_index(particles, weights, indexes)\n",
    "                assert np.allclose(weights, 1 / N)\n",
    "            mu, var = estimate(particles, weights)\n",
    "            xs.append(mu)\n",
    "\n",
    "            weights_trace.append(weights.copy())\n",
    "            particle_trace.append(particles.copy())\n",
    "\n",
    "            if do_plot:\n",
    "                if plot_particles:\n",
    "                    plt.scatter(\n",
    "                        particles[:, 0], particles[:, 1], color=\"k\", marker=\",\", s=0.5, alpha=0.2\n",
    "                    )\n",
    "                p1 = plt.scatter(obs[0], obs[1], marker=\"x\", color=\"r\")\n",
    "                p2 = plt.scatter(mu[0], mu[1], marker=\"s\", color=\"b\")\n",
    "\n",
    "            # print(f\"Step {i+1}/{len(data)}\")\n",
    "\n",
    "        xs = np.array(xs)\n",
    "        # plt.plot(xs[:, 0], xs[:, 1])\n",
    "        if do_plot:\n",
    "            plt.legend([p1, p2], [\"Actual\", \"PF\"], loc=4, numpoints=1)\n",
    "        # plt.xlim(-width / 2, width / 2)\n",
    "        # plt.ylim(-height / 2, height / 2)\n",
    "\n",
    "        pos_error = np.linalg.norm(mu[:2] - data[:, :2], axis=1)\n",
    "        # print(\"error, variance: \", pos_error.mean(), pos_error.var())\n",
    "        all_error = np.linalg.norm(xs[:, :] - data[:, :4], axis=1)\n",
    "        # print(\"all error, variance: \", all_error.mean(), all_error.var())\n",
    "\n",
    "        if do_plot:\n",
    "            plt.show()\n",
    "\n",
    "        return {\n",
    "            \"particles\": array(particle_trace),\n",
    "            \"weights\": array(weights_trace),\n",
    "            \"estimates\": xs,\n",
    "            \"pos_error\": pos_error,\n",
    "            \"pos_error_mean\": pos_error.mean(),\n",
    "            \"all_error\": all_error,\n",
    "            \"all_error_mean\": all_error.mean(),\n",
    "            \"truth\": data,\n",
    "        }\n",
    "\n",
    "    data.loc[:, \"RelativeTime\"] = (\n",
    "        data[\"BaseDateTime\"] - data[\"BaseDateTime\"].iloc[0]\n",
    "    ).dt.total_seconds()\n",
    "    assert data[\"RelativeTime\"].is_monotonic_increasing\n",
    "\n",
    "    ground_truth = stack(\n",
    "        (data[\"East\"], data[\"North\"], deg2rad(data[\"COG\"]), data[\"SOG\"], data[\"RelativeTime\"])\n",
    "    ).T\n",
    "\n",
    "    ground_truth[0, 4] = 0\n",
    "    ground_truth[1:, 4] = ground_truth[1:, 4] - ground_truth[:-1, 4]\n",
    "\n",
    "    seed(seed_value)\n",
    "    return run_pf1(N=2000, data=ground_truth, plot_particles=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# df_for_type = df[df[\"VesselTypeName\"].isin(relevant_vessel_types)]\n",
    "df_for_type = df[df[\"VesselTypeName\"].isin(landscapes.keys())]\n",
    "df_for_type = df_for_type[df_for_type[\"MMSI\"].isin(relevant_MMSIs)]\n",
    "\n",
    "for tau in np.arange(0.0, 1.1, 0.1):\n",
    "    for mmsi in relevant_MMSIs:\n",
    "        for seed_value in [2024]:\n",
    "            data = df_for_type[df_for_type[\"MMSI\"] == mmsi].copy()\n",
    "            if data.empty:\n",
    "                continue\n",
    "            vessel_type = data[\"VesselTypeName\"].iloc[0]\n",
    "\n",
    "            interpolator = interpolators[vessel_type]\n",
    "            experiment_result = run_experiment_pf1(\n",
    "                data,\n",
    "                constitutional_trust=tau,\n",
    "                seed_value=seed_value,\n",
    "                interpolator=interpolator,\n",
    "            )\n",
    "            results.append(\n",
    "                dict(\n",
    "                    mmsi=mmsi,\n",
    "                    type=vessel_type,\n",
    "                    tau=tau,\n",
    "                    seed=seed_value,\n",
    "                    **experiment_result,\n",
    "                )\n",
    "            )\n",
    "            print(\n",
    "                f\"Finished {mmsi} (progress: {len(results)}/{len(df_for_type['MMSI'].unique()) * 11})\"\n",
    "            )\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before = monotonic()\n",
    "# run_experiment_pf1(\n",
    "#     data[:20],\n",
    "#     constitutional_trust=0.5,\n",
    "#     seed_value=2024,\n",
    "#     interpolator=interpolator,\n",
    "#     full_inference=False,\n",
    "#     use_constitution=True,\n",
    "# )\n",
    "# after = monotonic()\n",
    "# after - before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_pickle(\"res-all-taus.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_pickle(\"res-all-taus-paper.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[\"type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[df_results[\"tau\"] != 0.0][\"pos_error_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with_benefit = df_results.groupby(\"mmsi\").apply(\n",
    "#     lambda group: group.assign(\n",
    "#         benefit=(\n",
    "#             group[group[\"tau\"] != 0.0][\"pos_error_mean\"].min()\n",
    "#             - group[group[\"tau\"] == 0.0][\"pos_error_mean\"].item()\n",
    "#         )\n",
    "#     ),\n",
    "#     include_groups=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_benefit = df_results.groupby(\"mmsi\").apply(\n",
    "    lambda group: group.copy()[\n",
    "        (group[\"tau\"] == 0.0)\n",
    "        | (group[\"pos_error_mean\"] == group[group[\"tau\"] != 0.0][\"pos_error_mean\"].min())\n",
    "    ]\n",
    "    .assign(\n",
    "        benefit=(\n",
    "            group[group[\"tau\"] != 0.0][\"pos_error_mean\"].min()\n",
    "            - group[group[\"tau\"] == 0.0][\"pos_error_mean\"].item()\n",
    "        )\n",
    "    )\n",
    "    .sort_values(\"tau\"),\n",
    "    include_groups=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with_benefit = with_benefit[\n",
    "#     (with_benefit[\"benefit\"] > max(with_benefit[\"benefit\"].quantile(0.6).item(), 0))\n",
    "# ]\n",
    "with_benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = with_benefit.loc[[with_benefit.index.get_level_values(0)[::2][10]]]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pf, result_cofi = results.iloc[0], results.iloc[1]\n",
    "result = result_cofi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "columns = [\"x\", \"y\", \"d_phi\", \"d_r\"]\n",
    "\n",
    "# time_start = 0\n",
    "# time_end = 400\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i >= result[\"particles\"].shape[2]:\n",
    "        break\n",
    "    ax.plot(result[\"particles\"][:, :, i], alpha=0.2)\n",
    "\n",
    "    if i < result_pf[\"estimates\"].shape[1]:\n",
    "        ax.plot(result_pf[\"estimates\"][:, i], c=\"black\", lw=2, label=\"Mean_plain\")\n",
    "    if i < result_cofi[\"estimates\"].shape[1]:\n",
    "        ax.plot(result_cofi[\"estimates\"][:, i], c=\"black\", lw=2, label=\"Mean_const\")\n",
    "\n",
    "    if i < result[\"truth\"].shape[1]:\n",
    "        ax.plot(result[\"truth\"][:, i], c=\"green\", lw=2, label=\"Ground Truth\")\n",
    "    ax.set_title(columns[i])\n",
    "\n",
    "    # ax.axvline(result[\"deviate_from\"], c=\"black\", ls=\"--\", lw=1)\n",
    "\n",
    "    if i == 0:\n",
    "        ax.set_ylim(-width, width)\n",
    "    if i == 1:\n",
    "        ax.set_ylim(-height, height)\n",
    "\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "\n",
    "axes[1, 0].axhline(2 * pi, c=\"black\", ls=\"--\", lw=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "landscape.scatter(s=2, alpha=0.5)\n",
    "plot_trajectories(result_pf[\"truth\"][:, :2], label=\"True trajectory\")\n",
    "plot_trajectories(result_pf[\"estimates\"][:, :2], label=\"Estimated trajectory (PF)\")\n",
    "plot_trajectories(result_cofi[\"estimates\"][:, :2], label=\"Estimated trajectory (CoFi)\")\n",
    "\n",
    "# show_n_particles = 10\n",
    "# plot_trajectories(trace_all_particles[:, :show_n_particles, :2].swapaxes(1, 0), label=None)\n",
    "\n",
    "scale = 1.5\n",
    "# plt.gca().set_xlim(-width / 2 * scale, width / 2 * scale)\n",
    "# plt.gca().set_ylim(-height / 2 * scale, height / 2 * scale)\n",
    "\n",
    "# plt.gca().set_xlim(-15_000, -2_000)\n",
    "# plt.gca().set_ylim(8_000, 18_000)\n",
    "\n",
    "all_coords = np.concatenate(\n",
    "    [result_pf[\"truth\"][:, :2], result_pf[\"estimates\"][:, :2], result_cofi[\"estimates\"][:, :2]]\n",
    ")\n",
    "plt.gca().set_xlim(all_coords[:, 0].min(), all_coords[:, 0].max())\n",
    "plt.gca().set_ylim(all_coords[:, 1].min(), all_coords[:, 1].max())\n",
    "\n",
    "# Move legend to the right of the plot\n",
    "plt.legend()\n",
    "move_legend(plt.gca(), loc=\"center left\", bbox_to_anchor=(1.3, 0.5))\n",
    "\n",
    "plt.colorbar()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_to_polar(coords: ndarray) -> ndarray:\n",
    "    collection = CartesianCollection(origin=origin, dimensions=1)\n",
    "    collection.append_with_default(coords, np.nan)\n",
    "    return collection.to_polar().coordinates()\n",
    "\n",
    "\n",
    "def make_line_string(cartesian_coords: ndarray) -> str:\n",
    "    return json.dumps(\n",
    "        {\n",
    "            \"type\": \"LineString\",\n",
    "            \"coordinates\": cartesian_to_polar(cartesian_coords).tolist(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "start_date = \"2023-08-01 0:0:00\"\n",
    "date_format = \"%Y-%m-%d %X\"\n",
    "\n",
    "\n",
    "def make_route_points(cartesian_coords: ndarray) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        cartesian_coords: Shape (T, 2)\n",
    "    \"\"\"\n",
    "    return json.dumps(\n",
    "        {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"features\": [\n",
    "                {\n",
    "                    \"type\": \"Feature\",\n",
    "                    \"geometry\": {\n",
    "                        \"type\": \"Point\",\n",
    "                        \"coordinates\": coord.tolist(),\n",
    "                    },\n",
    "                    \"properties\": {\n",
    "                        \"timestamp\": time,\n",
    "                    },\n",
    "                }\n",
    "                for coord, time in zip(\n",
    "                    cartesian_to_polar(cartesian_coords),\n",
    "                    pd.date_range(start=start_date, periods=cartesian_coords.shape[0]).strftime(\n",
    "                        date_format\n",
    "                    ),\n",
    "                )\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def make_particles(cartesian_coords: ndarray, weights: ndarray) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        cartesian_coords: Shape (T, N, 2)\n",
    "        weights: Shape (T, N)\n",
    "    \"\"\"\n",
    "    return json.dumps(\n",
    "        {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"features\": [\n",
    "                {\n",
    "                    \"type\": \"Feature\",\n",
    "                    \"geometry\": {\n",
    "                        \"type\": \"Point\",\n",
    "                        \"coordinates\": coord.tolist(),\n",
    "                    },\n",
    "                    \"properties\": {\n",
    "                        \"timestamp\": time,\n",
    "                        \"weight\": weight,\n",
    "                    },\n",
    "                }\n",
    "                for coords_t, weights_t, time in zip(\n",
    "                    cartesian_coords,\n",
    "                    weights,\n",
    "                    pd.date_range(start=start_date, periods=cartesian_coords.shape[0]).strftime(\n",
    "                        date_format\n",
    "                    ),\n",
    "                )\n",
    "                for coord, weight in zip(\n",
    "                    cartesian_to_polar(coords_t),\n",
    "                    weights_t.tolist(),\n",
    "                )\n",
    "            ],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pf[\"weights\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_particles = 4\n",
    "map = KeplerGl(\n",
    "    data={\n",
    "        \"True Trajectory Track\": make_line_string(result_pf[\"truth\"][:, :2]),\n",
    "        \"True Trajectory\": make_route_points(result_pf[\"truth\"][:, :2]),\n",
    "        #\n",
    "        \"Estimated Trajectory (CoFi) Track\": make_line_string(result_cofi[\"estimates\"][:, :2]),\n",
    "        \"Estimated Trajectory (CoFi)\": make_route_points(result_cofi[\"estimates\"][:, :2]),\n",
    "        #\n",
    "        \"Estimated Trajectory (PF) Track\": make_line_string(result_pf[\"estimates\"][:, :2]),\n",
    "        \"Estimated Trajectory (PF)\": make_route_points(result_pf[\"estimates\"][:, :2]),\n",
    "        #\n",
    "        \"Particles (PF)\": make_particles(\n",
    "            result_pf[\"particles\"][:, :subsample_particles, :2],\n",
    "            weights=result_pf[\"weights\"][:, :subsample_particles],\n",
    "        ),\n",
    "        \"Particles (CoFi)\": make_particles(\n",
    "            result_cofi[\"particles\"][:, :subsample_particles, :2],\n",
    "            weights=result_cofi[\"weights\"][:, :subsample_particles],\n",
    "        ),\n",
    "    },\n",
    "    config={\n",
    "        \"version\": \"v1\",\n",
    "        \"config\": {\n",
    "            \"mapState\": {\n",
    "                \"latitude\": cartesian_to_polar(all_coords)[:, 1].mean(),\n",
    "                \"longitude\": cartesian_to_polar(all_coords)[:, 0].mean(),\n",
    "                \"zoom\": 11,\n",
    "            },\n",
    "            \"visState\": {\n",
    "                \"filters\": [\n",
    "                    {\n",
    "                        \"dataId\": [data_field],\n",
    "                        \"id\": f\"94q7ipgwi-{data_field}\",\n",
    "                        \"name\": [\"timestamp\"],\n",
    "                        \"type\": \"DateTime\",\n",
    "                        \"yAxis\": None,\n",
    "                        \"speed\": 0.2,\n",
    "                    }\n",
    "                    for data_field in [\n",
    "                        \"True Trajectory\",\n",
    "                        \"Estimated Trajectory (CoFi)\",\n",
    "                        \"Estimated Trajectory (PF)\",\n",
    "                        \"Particles (PF)\",\n",
    "                        \"Particles (CoFi)\",\n",
    "                    ]\n",
    "                ]\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "map.save_to_html(file_name=\"debug_keplergl.html\")\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ = []\n",
    "for vessel_type in interpolators:\n",
    "    df_ = df[df[\"VesselTypeName\"] == vessel_type]\n",
    "    points = df_[[\"East\", \"North\"]].values\n",
    "    c = interpolators[vessel_type](points)\n",
    "    for value in c:\n",
    "        res_.append(\n",
    "            {\n",
    "                \"Vessel Type\": vessel_type,\n",
    "                \"Constitutional\": value.item(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "res_df = pd.DataFrame(res_)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(\n",
    "    data=res_df.replace({\"Vessel Type\": {\"Search and rescue vessels\": \"SAR\"}}),\n",
    "    x=\"Vessel Type\",\n",
    "    y=\"Constitutional\",\n",
    "    hue=\"Vessel Type\",\n",
    "    order=[\"Cargo\", \"Towing\", \"SAR\"],\n",
    "    hue_order=[\"Cargo\", \"Towing\", \"SAR\"],\n",
    ")\n",
    "sns.despine()\n",
    "plt.ylabel(r\"Average $P(c_t | \\mathbf{x}_t, \\mathbf{z}_t)$\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylim(0, 1)\n",
    "plt.savefig(Path(\".\") / \"plots\" / f\"ais-{setting}-constitution.pdf\", bbox_inches=\"tight\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_export = df_results.copy()[\n",
    "    [\"mmsi\", \"type\", \"tau\", \"seed\", \"pos_error_mean\", \"all_error_mean\"]\n",
    "]\n",
    "df_results_export.to_csv(\"res-all-taus-export.csv\", index=False)\n",
    "\n",
    "df_ = df_results_export.rename(\n",
    "    columns={\n",
    "        \"type\": \"Vessel Type\",\n",
    "        \"tau\": r\"$\\tau$\",\n",
    "        \"pos_error_mean\": \"Relative Position Error\",\n",
    "    }\n",
    ")\n",
    "df_ = df_[df_[\"Relative Position Error\"] < 100_000]\n",
    "reference_error = df_[(df_[\"Vessel Type\"] == \"Cargo\") & (df_[r\"$\\tau$\"] == 0.0)][\n",
    "    \"Relative Position Error\"\n",
    "].mean()\n",
    "df_[\"Relative Position Error\"] /= reference_error\n",
    "sns.barplot(\n",
    "    x=r\"$\\tau$\",\n",
    "    y=\"Relative Position Error\",\n",
    "    hue=\"Vessel Type\",\n",
    "    data=df_[df_[r\"$\\tau$\"].isin([0.0, 0.5, 1.0])],\n",
    "    hue_order=[\"Cargo\", \"Towing\", \"Search and rescue vessels\"],\n",
    ")\n",
    "\n",
    "# mvoe the legend outside\n",
    "sns.move_legend(plt.gca(), \"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.savefig(Path(\".\") / \"plots\" / \"relative_position_error.pdf\", bbox_inches=\"tight\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.barplot(x=\"tau\", y=\"all_error_mean\", hue=\"type\", data=df_results)\n",
    "sns.boxenplot(\n",
    "    x=\"tau\",\n",
    "    y=\"all_error_mean\",\n",
    "    hue=\"type\",\n",
    "    data=df_results,\n",
    "    # width_method=\"linear\",\n",
    ")\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=\"tau\", y=\"pos_error_mean\", hue=\"type\", data=df_results)\n",
    "plt.ylim(None, 30_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groub by MMSI and trhen take the tau with the lowest error each\n",
    "# df_results[[\"mmsi\", \"pos_error_mean\"]].groupby(\"mmsi\").apply(\n",
    "#     lambda x: x.sort_values(\"tau\").loc[x[\"pos_error_mean\"].idxmin()]\n",
    "# )\n",
    "res = []\n",
    "for mmis in df_results[\"mmsi\"].unique():\n",
    "    df_ = df_results[df_results[\"mmsi\"] == mmis]\n",
    "    # sort by tau\n",
    "    df_ = df_.sort_values(\"pos_error_mean\")\n",
    "    res.append((df_[\"type\"].iloc[0], df_[\"tau\"].iloc[0]))\n",
    "\n",
    "res = pd.DataFrame(res, columns=[\"Vessel Type\", r\"$\\tau$\"])\n",
    "# Rname Search and rescue vessels to SAR\n",
    "res[\"Vessel Type\"] = res[\"Vessel Type\"].replace(\"Search and rescue vessels\", \"SAR\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_ = res.copy()\n",
    "\n",
    "# r_[\"Vessel Type\"] = \"All\"\n",
    "# res_ = pd.concat([res, r_], axis=0)\n",
    "# res_.sort_values(\"Vessel Type\", inplace=True)\n",
    "\n",
    "sns.histplot(\n",
    "    r_,\n",
    "    x=r\"$\\tau$\",\n",
    "    hue=\"Vessel Type\",\n",
    "    multiple=\"stack\",\n",
    "    bins=10,\n",
    "    stat=\"percent\",\n",
    "    shrink=0.75,\n",
    "    # palette=\"colorblind\",\n",
    ")\n",
    "\n",
    "plt.ylabel(\"\")\n",
    "plt.yticks(plt.yticks()[0], [f\"{i.get_text()}%\" for i in plt.yticks()[1]])\n",
    "sns.despine()\n",
    "\n",
    "# Fix the placmenet of the x-axis\n",
    "plt.xticks(np.arange(0, 1.1, 0.1) + 0.05, [f\"{i:.1f}\" for i in np.arange(0, 1.1, 0.1)])\n",
    "\n",
    "plt.savefig(Path(\".\") / \"plots\" / \"tau-distribution.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    df_results,\n",
    "    x=\"pos_error_mean\",  # or \"all_error\"\n",
    "    col=\"tau\",\n",
    "    col_wrap=3,\n",
    "    facet_kws=dict(margin_titles=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Older stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "df_for_type = df[df[\"VesselTypeName\"].isin([\"Cargo\", \"Cargo hazardous\"])]\n",
    "\n",
    "for tau in [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]:\n",
    "    for mmsi in relevant_MMSIs:\n",
    "        for seed_value in [2024]:\n",
    "            data = df_for_type[df_for_type[\"MMSI\"] == mmsi].copy()\n",
    "            if data.empty:\n",
    "                continue\n",
    "            vessel_type = data[\"VesselTypeName\"].iloc[0]\n",
    "            experiment_result = run_experiment(\n",
    "                data, constitutional_trust=tau, seed_value=seed_value, verbose=False\n",
    "            )\n",
    "            results.append(\n",
    "                dict(\n",
    "                    mmsi=mmsi,\n",
    "                    type=vessel_type,\n",
    "                    tau=tau,\n",
    "                    seed=seed_value,\n",
    "                    mean_mean=np.mean(experiment_result[\"position_accuracy_mean\"]),\n",
    "                    mean_std=np.std(experiment_result[\"position_accuracy_mean\"]),\n",
    "                    map_mean=np.mean(experiment_result[\"position_accuracy_map\"]),\n",
    "                    map_std=np.std(experiment_result[\"position_accuracy_map\"]),\n",
    "                    **experiment_result,\n",
    "                )\n",
    "            )\n",
    "            print(f\"Finished {mmsi} (progress: {len(results)}/{len(df_for_type['MMSI'].unique())})\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    df_results,  # [[\"map_mean\", \"tau\"]],\n",
    "    x=\"mean_mean\",\n",
    "    col=\"tau\",\n",
    "    col_wrap=3,\n",
    "    facet_kws=dict(margin_titles=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(\n",
    "    data=df_results,\n",
    "    kind=\"bar\",\n",
    "    x=\"tau\",\n",
    "    y=\"mean_mean\",\n",
    "    # hue=\"sex\",\n",
    "    errorbar=\"sd\",\n",
    "    alpha=0.6,\n",
    "    height=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the weights of the particles\n",
    "# plt.plot(result[\"trace_weight_e1\"])\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(result[\"trace_weight_e2\"])\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the weights of the particles\n",
    "# plt.plot(result[\"trace_weights\"])\n",
    "plt.plot(result[\"trace_weights_unnormalized\"])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(result[\"trace_time\"])\n",
    "plt.plot(np.diff(result[\"trace_time\"]) / 60)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(trace_all_particles[20, :, 2].ravel(), bins=100)\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "landscape.scatter(s=0.1, alpha=0.5)\n",
    "plot_trajectories(result[\"ground_truth\"], label=\"True trajectory\")\n",
    "plot_trajectories(result[\"trace_state\"][:, :2], label=\"Estimated trajectory\")\n",
    "\n",
    "# show_n_particles = 10\n",
    "# plot_trajectories(trace_all_particles[:, :show_n_particles, :2].swapaxes(1, 0), label=None)\n",
    "\n",
    "scale = 1.5\n",
    "plt.gca().set_xlim(-width / 2 * scale, width / 2 * scale)\n",
    "plt.gca().set_ylim(-height / 2 * scale, height / 2 * scale)\n",
    "\n",
    "plt.colorbar()\n",
    "plt.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "landscape.scatter(s=0.1, alpha=0.2)\n",
    "# plt.scatter(*result[\"trace_all_particles\"][:, :, :2][50, ...].T, c=\"r\", s=1)\n",
    "plt.scatter(*debug_df[[\"East\", \"North\"]].values.T, c=\"r\", s=1)\n",
    "plt.xlim(-width / 2 * scale, width / 2 * scale)\n",
    "plt.ylim(-height / 2 * scale, height / 2 * scale)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 30\n",
    "\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "\n",
    "# draw a box around the frame (-1500, -1500) to (1500, 1500)\n",
    "plt.plot(\n",
    "    [-width / 2, width / 2, width / 2, -width / 2, -width / 2],\n",
    "    [-height / 2, -height / 2, height / 2, height / 2, -height / 2],\n",
    "    \"k-\",\n",
    "    alpha=0.3,\n",
    "    label=\"Frame\",\n",
    ")\n",
    "\n",
    "plot_trajectories(result[\"ground_truth\"], label=\"True trajectory\", color=\"k\", alpha=0.3)\n",
    "\n",
    "plt.hist2d(\n",
    "    *result[\"trace_all_particles\"][index, :, :2].T,\n",
    "    bins=50,\n",
    "    range=[[-width / 2 * scale, width / 2 * scale], [-height / 2 * scale, height / 2 * scale]],\n",
    "    cmap=\"Oranges\",\n",
    ")\n",
    "\n",
    "if index > 0:\n",
    "    # Plot the ship position and position estimate at that time point\n",
    "    plt.gca().plot(\n",
    "        *result[\"ground_truth\"][index - 1],\n",
    "        marker=ship_marker,\n",
    "        markersize=14,\n",
    "        label=\"True position\",\n",
    "        c=\"k\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    plt.gca().plot(\n",
    "        *result[\"trace_state\"][index - 1, :2],\n",
    "        marker=ship_marker,\n",
    "        markersize=14,\n",
    "        label=\"Estimated position\",\n",
    "        c=\"b\",\n",
    "    )\n",
    "\n",
    "\n",
    "plt.legend(\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(1, 0.5),\n",
    ")\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_starmap = pd.DataFrame(\n",
    "    [\n",
    "        dict(name=r\"StarMap ($100 \\times x 100$)\", time=640.4149391481187),\n",
    "        dict(name=r\"StarMap ($100 \\times x 100$)\", time=627.0258562378585),\n",
    "        dict(name=r\"StarMap ($100 \\times x 100$)\", time=624.6253640819341),\n",
    "        #\n",
    "        dict(name=r\"StarMap ($50 \\times 50$)\", time=495.63382973195985),\n",
    "        dict(name=r\"StarMap ($50 \\times 50$)\", time=487.28305474994704),\n",
    "        dict(name=r\"StarMap ($50 \\times 50$)\", time=487.8447328049224),\n",
    "    ]\n",
    ")\n",
    "res_perf = pd.DataFrame(\n",
    "    [\n",
    "        dict(name=\"Particle Filter\", time=0.03765651909634471 / 20),\n",
    "        dict(name=\"Particle Filter\", time=0.0383494570851326 / 20),\n",
    "        dict(name=\"Particle Filter\", time=0.03719700896181166 / 20),\n",
    "        #\n",
    "        dict(name=\"CoFi\", time=113.63170561008155 / 20),\n",
    "        dict(name=\"CoFi\", time=113.54428059794009 / 20),\n",
    "        dict(name=\"CoFi\", time=113.22225885814987 / 20),\n",
    "        #\n",
    "        dict(name=\"CoFi Precomputed\", time=0.051308222813531756 / 20),\n",
    "        dict(name=\"CoFi Precomputed\", time=0.04675357579253614 / 20),\n",
    "        dict(name=\"CoFi Precomputed\", time=0.05256968503817916 / 20),\n",
    "    ]\n",
    ")\n",
    "cocktail = pd.DataFrame(\n",
    "    [\n",
    "        dict(name=r\"StarMap ($100 \\times 100$)\", time=640.4149391481187),\n",
    "        dict(name=r\"StarMap ($100 \\times 100$)\", time=627.0258562378585),\n",
    "        dict(name=r\"StarMap ($100 \\times 100$)\", time=624.6253640819341),\n",
    "        #\n",
    "        dict(name=r\"StarMap ($50 \\times 50$)\", time=495.63382973195985),\n",
    "        dict(name=r\"StarMap ($50 \\times 50$)\", time=487.28305474994704),\n",
    "        dict(name=r\"StarMap ($50 \\times 50$)\", time=487.8447328049224),\n",
    "        #\n",
    "        dict(name=\"CoFi\\n10 iterations\", time=113.63170561008155 / 20 * 10),\n",
    "        dict(name=\"CoFi\\n10 iterations\", time=113.54428059794009 / 20 * 10),\n",
    "        dict(name=\"CoFi\\n10 iterations\", time=113.22225885814987 / 20 * 10),\n",
    "    ]\n",
    ")\n",
    "cocktail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=cocktail, x=\"name\", y=\"time\", hue=\"name\", palette=\"colorblind\")\n",
    "plt.xlabel(\"\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
